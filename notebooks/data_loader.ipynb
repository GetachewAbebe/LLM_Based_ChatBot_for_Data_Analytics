{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "# Adding paths to access the custom utility functions\n",
    "sys.path.append(os.path.abspath(os.path.join('../utils')))\n",
    "from utils import run_sql_query, populate_dataframe_to_database, create_table_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:36,207 - INFO - Successfully connected to the PostgreSQL database.\n"
     ]
    }
   ],
   "source": [
    "# Get the database connection parameters from environment variables\n",
    "DB_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "DB_PORT = os.getenv(\"POSTGRES_PORT\")\n",
    "DB_NAME = os.getenv(\"POSTGRES_DATABASE\")\n",
    "DB_USER = os.getenv(\"POSTGRES_USERNAME\")\n",
    "DB_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "# Connect to the PostgreSQL database\n",
    "try:\n",
    "    connection_params = {\n",
    "        'host': \"localhost\",\n",
    "        'port': DB_PORT,\n",
    "        'database': DB_NAME,\n",
    "        'user': DB_USER,\n",
    "        'password': DB_PASSWORD\n",
    "    }\n",
    "    connection = psycopg2.connect(**connection_params)\n",
    "    logger.info(\"Successfully connected to the PostgreSQL database.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logger.error(\"Error connecting to PostgreSQL database: %s\", e)\n",
    "    raise Exception(\"Unable to connect to the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory\n",
    "root_directory = '../data/'\n",
    "# Initialize SQL queries for schema creation\n",
    "sql_queries = \"\"\n",
    "# Initialize a DataFrame to hold all Totals data\n",
    "all_totals_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_changer(df):\n",
    "    \"\"\"\n",
    "    Converts parsable dates in a DataFrame's 'Date' column to datetime format and drops rows with non-parsable dates.\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing a 'Date' column.\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the 'Date' column converted to datetime format (if successful).\n",
    "    \"\"\"\n",
    "    # Setting up the logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()  # Logs to console\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    try:\n",
    "        # Attempt to convert parsable dates to datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "        # Drop rows with non-parsable dates (converted to NaNs)\n",
    "        df = df.dropna(subset=['Date'])\n",
    "        logger.info(\"Successfully converted dates and dropped rows with non-parsable dates.\")\n",
    "        return df\n",
    "    except pd.errors.ParserError as err:\n",
    "        logger.error(f\"Error parsing date column: {err}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:36,802 - INFO - Processing folder: content_type\n",
      "2025-02-23 23:15:36,820 - INFO - Generated table query for content_type_chart_data: CREATE TABLE IF NOT EXISTS content_type_chart_data (\n",
      "                \"Date\" TEXT, \"Content type\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Content type\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:37,675 - INFO - Generated table query for content_type_table_data: CREATE TABLE IF NOT EXISTS content_type_table_data (\n",
      "                \"Content type\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Content type\")\n",
      "            );\n",
      "2025-02-23 23:15:37,854 - INFO - Processing folder: geography\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 2558 rows into the database table content_type_chart_data.\n",
      "Log success\n",
      "Inserted 3 rows into the database table content_type_table_data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:37,883 - INFO - Generated table query for geography_chart_data: CREATE TABLE IF NOT EXISTS geography_chart_data (\n",
      "                \"Date\" TEXT, \"Geography\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Geography\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:38,671 - INFO - Generated table query for geography_table_data: CREATE TABLE IF NOT EXISTS geography_table_data (\n",
      "                \"Geography\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Geography\")\n",
      "            );\n",
      "2025-02-23 23:15:38,844 - INFO - Processing folder: viewership_by_date\n",
      "2025-02-23 23:15:38,847 - INFO - Generated table query for viewership_by_date_table_data: CREATE TABLE IF NOT EXISTS viewership_by_date_table_data (\n",
      "                \"Date\" TEXT, \"Views\" DOUBLE PRECISION, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Date\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 37091 rows into the database table geography_chart_data.\n",
      "Log success\n",
      "Inserted 30 rows into the database table geography_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:39,080 - INFO - Processing folder: cities\n",
      "2025-02-23 23:15:39,104 - INFO - Generated table query for cities_chart_data: CREATE TABLE IF NOT EXISTS cities_chart_data (\n",
      "                \"Date\" TEXT, \"Cities\" TEXT, \"City name\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Cities\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 502 rows into the database table viewership_by_date_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:39,372 - INFO - Generated table query for cities_table_data: CREATE TABLE IF NOT EXISTS cities_table_data (\n",
      "                \"Cities\" TEXT, \"City name\" TEXT, \"Geography\" TEXT, \"Geography.1\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Cities\")\n",
      "            );\n",
      "2025-02-23 23:15:39,530 - INFO - Processing folder: traffic_source\n",
      "2025-02-23 23:15:39,554 - INFO - Generated table query for traffic_source_chart_data: CREATE TABLE IF NOT EXISTS traffic_source_chart_data (\n",
      "                \"Date\" TEXT, \"Traffic source\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Traffic source\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 6395 rows into the database table cities_chart_data.\n",
      "Log success\n",
      "Inserted 9 rows into the database table cities_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:39,862 - INFO - Generated table query for traffic_source_table_data: CREATE TABLE IF NOT EXISTS traffic_source_table_data (\n",
      "                \"Traffic source\" TEXT, \"Views\" DOUBLE PRECISION, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT, \"Impressions\" DOUBLE PRECISION, \"Impressions click-through rate (%)\" DOUBLE PRECISION,\n",
      "                PRIMARY KEY (\"Traffic source\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 11511 rows into the database table traffic_source_chart_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:40,070 - INFO - Processing folder: subscription_source\n",
      "2025-02-23 23:15:40,090 - INFO - Generated table query for subscription_source_chart_data: CREATE TABLE IF NOT EXISTS subscription_source_chart_data (\n",
      "                \"Date\" TEXT, \"Subscription source\" TEXT, \"Subscribers\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Subscription source\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 11 rows into the database table traffic_source_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:40,482 - INFO - Generated table query for subscription_source_table_data: CREATE TABLE IF NOT EXISTS subscription_source_table_data (\n",
      "                \"Subscription source\" TEXT, \"Subscribers\" INTEGER, \"Subscribers gained\" INTEGER, \"Subscribers lost\" INTEGER,\n",
      "                PRIMARY KEY (\"Subscription source\")\n",
      "            );\n",
      "2025-02-23 23:15:40,648 - INFO - Processing folder: viewer_gender\n",
      "2025-02-23 23:15:40,651 - INFO - Generated table query for viewer_gender_table_data: CREATE TABLE IF NOT EXISTS viewer_gender_table_data (\n",
      "                \"Viewer gender\" TEXT, \"Views (%)\" DOUBLE PRECISION, \"Average view duration\" TEXT, \"Average percentage viewed (%)\" DOUBLE PRECISION, \"Watch time (hours) (%)\" DOUBLE PRECISION,\n",
      "                PRIMARY KEY (\"Viewer gender\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10232 rows into the database table subscription_source_chart_data.\n",
      "Log success\n",
      "Inserted 9 rows into the database table subscription_source_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:40,851 - INFO - Processing folder: device_type\n",
      "2025-02-23 23:15:40,859 - INFO - Generated table query for device_type_chart_data: CREATE TABLE IF NOT EXISTS device_type_chart_data (\n",
      "                \"Date\" TEXT, \"Device type\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Device type\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 2 rows into the database table viewer_gender_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:41,103 - INFO - Generated table query for device_type_table_data: CREATE TABLE IF NOT EXISTS device_type_table_data (\n",
      "                \"Device type\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Device type\")\n",
      "            );\n",
      "2025-02-23 23:15:41,271 - INFO - Processing folder: subtitles_and_cc\n",
      "2025-02-23 23:15:41,283 - INFO - Generated table query for subtitles_and_cc_chart_data: CREATE TABLE IF NOT EXISTS subtitles_and_cc_chart_data (\n",
      "                \"Date\" TEXT, \"Subtitles and CC\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Subtitles and CC\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 5116 rows into the database table device_type_chart_data.\n",
      "Log success\n",
      "Inserted 5 rows into the database table device_type_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:41,641 - INFO - Generated table query for subtitles_and_cc_table_data: CREATE TABLE IF NOT EXISTS subtitles_and_cc_table_data (\n",
      "                \"Subtitles and CC\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Subtitles and CC\")\n",
      "            );\n",
      "2025-02-23 23:15:41,798 - INFO - Processing folder: sharing_service\n",
      "2025-02-23 23:15:41,810 - INFO - Generated table query for sharing_service_chart_data: CREATE TABLE IF NOT EXISTS sharing_service_chart_data (\n",
      "                \"Date\" TEXT, \"Sharing service\" TEXT, \"Shares\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Sharing service\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10232 rows into the database table subtitles_and_cc_chart_data.\n",
      "Log success\n",
      "Inserted 9 rows into the database table subtitles_and_cc_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:42,125 - INFO - Generated table query for sharing_service_table_data: CREATE TABLE IF NOT EXISTS sharing_service_table_data (\n",
      "                \"Sharing service\" TEXT, \"Shares\" INTEGER,\n",
      "                PRIMARY KEY (\"Sharing service\")\n",
      "            );\n",
      "2025-02-23 23:15:42,310 - INFO - Processing folder: operating_system\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 11511 rows into the database table sharing_service_chart_data.\n",
      "Log success\n",
      "Inserted 10 rows into the database table sharing_service_table_data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:42,330 - INFO - Generated table query for operating_system_chart_data: CREATE TABLE IF NOT EXISTS operating_system_chart_data (\n",
      "                \"Date\" TEXT, \"Operating system\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Operating system\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:42,865 - INFO - Generated table query for operating_system_table_data: CREATE TABLE IF NOT EXISTS operating_system_table_data (\n",
      "                \"Operating system\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Operating system\")\n",
      "            );\n",
      "2025-02-23 23:15:43,030 - INFO - Processing folder: subscription_status\n",
      "2025-02-23 23:15:43,034 - INFO - Generated table query for subscription_status_chart_data: CREATE TABLE IF NOT EXISTS subscription_status_chart_data (\n",
      "                \"Date\" TEXT, \"Subscription status\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"Subscription status\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 20464 rows into the database table operating_system_chart_data.\n",
      "Log success\n",
      "Inserted 17 rows into the database table operating_system_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:43,257 - INFO - Generated table query for subscription_status_table_data: CREATE TABLE IF NOT EXISTS subscription_status_table_data (\n",
      "                \"Subscription status\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"Subscription status\")\n",
      "            );\n",
      "2025-02-23 23:15:43,412 - INFO - Processing folder: new_and_returning_viewers\n",
      "2025-02-23 23:15:43,418 - INFO - Generated table query for new_and_returning_viewers_chart_data: CREATE TABLE IF NOT EXISTS new_and_returning_viewers_chart_data (\n",
      "                \"Date\" TEXT, \"New and returning viewers\" TEXT, \"Views\" INTEGER,\n",
      "                PRIMARY KEY (\"Date\", \"New and returning viewers\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 2558 rows into the database table subscription_status_chart_data.\n",
      "Log success\n",
      "Inserted 3 rows into the database table subscription_status_table_data.\n",
      "Log success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:43,667 - INFO - Generated table query for new_and_returning_viewers_table_data: CREATE TABLE IF NOT EXISTS new_and_returning_viewers_table_data (\n",
      "                \"New and returning viewers\" TEXT, \"Views\" INTEGER, \"Watch time (hours)\" DOUBLE PRECISION, \"Average view duration\" TEXT,\n",
      "                PRIMARY KEY (\"New and returning viewers\")\n",
      "            );\n",
      "2025-02-23 23:15:43,855 - INFO - Processing folder: viewer_age\n",
      "2025-02-23 23:15:43,859 - INFO - Generated table query for viewer_age_table_data: CREATE TABLE IF NOT EXISTS viewer_age_table_data (\n",
      "                \"Viewer age\" TEXT, \"Views (%)\" DOUBLE PRECISION, \"Average view duration\" TEXT, \"Average percentage viewed (%)\" DOUBLE PRECISION, \"Watch time (hours) (%)\" DOUBLE PRECISION,\n",
      "                PRIMARY KEY (\"Viewer age\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3837 rows into the database table new_and_returning_viewers_chart_data.\n",
      "Log success\n",
      "Inserted 4 rows into the database table new_and_returning_viewers_table_data.\n",
      "Log success\n",
      "Inserted 4 rows into the database table viewer_age_table_data.\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each folder in the root directory\n",
    "for folder_name in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder_name)\n",
    "    logger.info(f\"Processing folder: {folder_name}\")\n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        chart_data_path = os.path.join(folder_path, 'Chart data.csv')\n",
    "        table_data_path = os.path.join(folder_path, 'Table data.csv')\n",
    "        totals_data_path = os.path.join(folder_path, 'Totals.csv')\n",
    "        # Process \"Chart data.csv\"\n",
    "        if os.path.exists(chart_data_path):\n",
    "            try:\n",
    "                chart_data_df = pd.read_csv(chart_data_path)\n",
    "                table_name = f'{folder_name.lower().replace(\" \",\"_\")}_chart_data'\n",
    "                table_query = create_table_query(chart_data_df, table_name)\n",
    "                sql_queries += table_query\n",
    "                logger.info(f\"Generated table query for {table_name}: {table_query.strip()}\")\n",
    "                # Create the table and populate data\n",
    "                run_sql_query(connection_params, table_query)\n",
    "                chart_data_df.fillna(0, inplace=True)\n",
    "                populate_dataframe_to_database(connection_params, chart_data_df, table_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {chart_data_path}: {e}\")\n",
    "        # Process \"Table data.csv\"\n",
    "        if os.path.exists(table_data_path):\n",
    "            try:\n",
    "                table_data_df = pd.read_csv(table_data_path)\n",
    "                table_name = f'{folder_name.lower().replace(\" \",\"_\")}_table_data'\n",
    "                table_query = create_table_query(table_data_df, table_name)\n",
    "                sql_queries += table_query\n",
    "                logger.info(f\"Generated table query for {table_name}: {table_query.strip()}\")\n",
    "                # Create the table and populate data\n",
    "                run_sql_query(connection_params, table_query)\n",
    "                table_data_df.fillna(0, inplace=True)\n",
    "                populate_dataframe_to_database(connection_params, table_data_df, table_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {table_data_path}: {e}\")\n",
    "        # Process \"Totals.csv\"\n",
    "        if os.path.exists(totals_data_path):\n",
    "            try:\n",
    "                totals_df = pd.read_csv(totals_data_path)\n",
    "                all_totals_data = pd.concat([all_totals_data, totals_df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {totals_data_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:44,030 - INFO - Generated table query for totals_table_data: CREATE TABLE IF NOT EXISTS totals_table_data (\n",
      "                \"Date\" TEXT, \"Views\" DOUBLE PRECISION, \"Subscribers\" DOUBLE PRECISION, \"Shares\" DOUBLE PRECISION,\n",
      "                PRIMARY KEY (\"Date\")\n",
      "            );\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log success\n",
      "Inserted 15348 rows into the database table totals_table_data.\n"
     ]
    }
   ],
   "source": [
    "# Create the CREATE TABLE query for totals_table_data\n",
    "if not all_totals_data.empty:\n",
    "    totals_table_query = create_table_query(all_totals_data, 'totals_table_data')\n",
    "    sql_queries += totals_table_query\n",
    "    logger.info(f\"Generated table query for totals_table_data: {totals_table_query.strip()}\")\n",
    "    # Execute the query to create the table\n",
    "    run_sql_query(connection_params, totals_table_query)\n",
    "    # Populate the combined Totals data into totals_table_data\n",
    "    populate_dataframe_to_database(connection_params, all_totals_data, 'totals_table_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:15:44,618 - INFO - SQL queries saved to ../database/db_schema.sql\n"
     ]
    }
   ],
   "source": [
    "# Specify the file path for the SQL file\n",
    "sql_file_path = '../database/db_schema.sql'\n",
    "\n",
    "# Write the SQL queries to the file\n",
    "os.makedirs(os.path.dirname(sql_file_path), exist_ok=True)\n",
    "with open(sql_file_path, 'w') as sql_file:\n",
    "    sql_file.write(sql_queries)\n",
    "\n",
    "logger.info(f\"SQL queries saved to {sql_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
